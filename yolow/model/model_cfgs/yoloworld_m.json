{
    "yoloworld_data_preprocessor": {
        "mean": [0, 0, 0],
        "std": [255, 255, 255],
        "bgr_to_rgb": true,
        "non_blocking": true
    },
    "yolov8_backbone": {
        "last_stage_out_channels": 768,
        "deepen_factor": 0.67,
        "widen_factor": 0.75
    },
    "yoloworld_text": {
        "model_name": "openai/clip-vit-base-patch32",
        "frozen_modules": ["all"],
        "channels": 512
    },
    "yoloworld_neck": {
        "in_channels": [256, 512, 768],
        "out_channels": [256, 512, 768],
        "embed_channels": [128, 256, 384],
        "num_heads": [4, 8, 12]
    },
    "yoloworld_backbone": {
        "with_text_model": true
    },
    "yoloworld_head_module": {
        "use_bn_head": true,
        "use_einsum": true
    },
    "yoloworld_detector": {
        "num_train_classes": 80,
        "num_test_classes": 1203,
        "mm_neck": true,
        "use_syncbn": true
    }
}
